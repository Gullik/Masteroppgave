
\section{Multigrid}
	\label{sec:multigrid}
	Here we will go through the main theory and
	algorithm behind the multigrid solver, developed as a part of this thesis.
	See also \citep{press_numerical_1988,trottenberg_multigrid_2000}.
	This solver is developed for the wholly distributed storage model.

	\subsection{General idea}
		\label{sec:mg_impl}
		An iterative solver solves a problem by starting with an initial guess, then it performs an algorithm
		improving the guess and repeats with the improved guess. The difference between
		the guess and the correct solution, the residual, does not necessarily converge equally fast for different frequencies.
		An iterative solver can be very efficient on reducing the local error,
		while the errors due to distant influence is reduced slowly.
		A multigrid solver attacks this problem by applying iterative methods on different discretizations of the problem, by solving on a very coarse grids
		the error due to distant influence will be reduced faster, while solving on a fine grid reduces the local error fast. So by solving on both
		fine and coarse grids the needed cycles will be reduced. To implement a multigrid algorithm we then need algorithms to solve the problem on a grid ,
		restriction and prolongation, see \cref{sec:GSRB,sec:restr_simplesec:prol_simple}, operators to transfer the problem between grids, as well as a method to compute the residual.


	\subsection{Algorithm}
	We want to solve a linear elliptic problem,
		\begin{align}
			\mathcal{L} u = f
		\end{align}
	where \(\mathcal{L}\) is a linear operator, \(u\) is the solution and \(f\) is
	a source term. In our specific case the operator is given by the Laplacian, the
	source term is given by the charge density and we solve for the electric potential.

	We discretize the equation onto a grid of size \(q\).
	\begin{align}
		\mathcal{L}_q u_q &= f_q \label{eq:difference}
	\end{align}

	Let the error, \(v_q\) be the difference between the exact solution and an approximate
	solution to the difference equation (\ref{eq:difference}), \( v_q = u_q - \tilde{u}_q \).
	Then we define the residual as what is left after using the approximate solution
	in the equation.

	\begin{align}
		d_q &= \mathcal{L}_q \tilde{u}_q - f_q
	\end{align}
	%
	Since \(\mathcal{L}\) is a linear operator the error satisfies the following relation

	\begin{align}
		\mathcal{L}_q v_q &= \mathcal{L}(u_q - \tilde{u}_q)  + (f_q- f_q)
		\\
		\mathcal{L}_q v_q &= - d_q \label{eq:diff_MG}
	\end{align}
	%
	The system can then be solved directly on this level with a chosen discretization.
	If we then increase the resolution to obtain a better solution, the system
	becomes harder to solve. The multigrid method approaches this problem
	by solving it on several different discretizations levels.
	We set up a systems of nested coarser regular grids,
	\(\mathfrak{T}_0 \supset \mathfrak{T}_{1} \supset \cdots \supset \mathfrak{T}_\ell\),
	where \(\mathfrak{T}_0\) is the finest and \(\mathfrak{T}_\ell\) is the coarsest grid.
	Then an iterative solver, which has the property of quickly converging of high frequency errors, i.e.
	local errors, is used on the finest grid. The remaining error is then transferred
	to a coarser grid where lower frequency errors are more easily found. The errors
	found on the coarser levels are then transferred up to the finest level as a correction.
	To transfer between the discretization coarseness we use restriction, \( \mathcal{R} \),
	and prolongation, \( \mathcal{P} \), operators. Due to the fewer grid points the problem is
	faster to solve on the coarser grid levels than on the fine grid.
	%
	Applying the restriction and prolongation operators on the grid gives us
	the grid discretized on a different level.

	\begin{align}
		\mathcal{R} d_q = d_{q+1} \qquad \text{and} \qquad \mathcal{P} d_q = d_{q - 1}
	\end{align}

	% %Figure
	\begin{figure}
	    \center
		\input{tikz/mgV}
		\caption{Schematic overview of the Multigrid cycle. In a three level MG implementation,
		there is 5 main steps in a cycle that needs to be considered.}
		\label{fig:MG_schematic}
	\end{figure}

	\cref{fig:MG_schematic} shows a schematic overview of a \(3\)-level
	version of a multigrid V cycle. The needed operations on each
	level is described in greater detail in \cref{sec:mg_impl}.

	\subsubsection{V-Cycle}
		The simplest multigrid cycle is called a V-cycle, which starts at the finest grid, goes down to the coarsest grid and then goes back up
		to the finest grid.	First the problem is smoothed on the finest level, then we compute the residual, or the rest after inserting the guess solution
		in the equation. The residual is then used as the source term for the next level, and we restrict it down as the source term for the next
		coarser level and repeat untill we reach the coarsest level. When we reach the coarsest level the problem is solved there and we obtain a correction
		term. The correction term is prolongated to the next finer level and added to the solution there, improving the solution, following by a new smoothing
		to obtain a new correction. This continue untill we reach the finest level again and a multigrid cycle is completed, see \cref{fig:MG_schematic} for a 3
		level schematic.

		In the following description of the steps in the MG method, we will use \(\phi\), \(\rho\), \(d\) and \(\omega\) to signify the solution, source,
		defect and correction respectively. A subscript means the grid level, where \(0\) si the finest level, while the superscript \(0\) implies an initial guess is used. Hats and tildes are also
	 	used to signify the stage the solution is in, with a hat meaning the solution is smoothed and a tilde meaning the correction from the grid below is added.

		The overarching algorithm is shown in \cref{algo:mg_recursive}
		\begin{algorithm}
			\caption{Multigrid V cycle}
	        \begin{algorithmic}
				\If {level = coarest}
				\State
				\begin{tabular}	{l | c}
					Solve &\( \widehat{\phi}_l = \mathcal{S}(\phi_l, \rho_l)\)
					\\
					Interpolate correction &\( \omega_{l-1} = \mathcal{I} \phi_l\)
				\end{tabular}
				\Else
	            \For{each level}
					\State
					\begin{tabular}	{l | c}
						Smooth &\( \widehat{\phi}_l = \mathcal{S}(\phi_l, \rho_l)\)
						\\
						Residual &	\(d_l = \nabla^2\widehat{\phi}_l - \rho_l\)
						\\
						Restrict &\(\rho_{l+1} = \mathcal{R}d_l \) \nonumber
						\\
						Go down, receive correction & \(\omega_l = \text{MG}( \phi_{l+1}\))
						\\
						Add correction	&\(\widetilde{\phi}_l = \widehat{\phi}_l + \omega_l\)
						\\
						Smooth	&\(\phi_l = \mathcal{S}(\widetilde{\phi}_l, \rho_l)\)
						\\
						Interpolate correction &\( \omega_{l-1} = \mathcal{I} \phi_l\)
					\end{tabular}
	            \EndFor
				\EndIf
	        \end{algorithmic}
			\label{algo:mg_recursive}
	    \end{algorithm}


		\begin{figure}
			\centering
			\include{tikz/mgAlgorithm}
			\caption{The functions used in \(2\) grid deep multigrid. The algorithm follows the }
		\end{figure}

		At the coarsest level the the problem is solved directly and the correction is propageted upward.

	\subsubsection{W Cycle}
		The W-cycle is similar to the V-cycle, with the difference that it spends longer than on the coarser grids,
		obtaining a better solution before returning to the finest grid.

	\subsubsection{Full Multigrid}
		Full Multigrid (FMG) is a multigrid cycle where the source term is known at all the levels.
		This is achieved usually by an interpolation scheme, or reuse the restriction algorithm,
		on the source term. Then the problem is first solved at the coarsest level before going up to the finest level.

	\subsection{Smoothing}
		% (NOTE TO SELF: Mention 'chebyshev' polynomial as smoothing! And 'Lim' See zhukov)
		The multigrid method prefers iterative solvers as smoothers which converges fast for high frequency
		errors. The low frequency convergence, i.e. the long range interaction, is improved
		by also solving it with coarser discretization. In this project we
		also wanted smoothers with good parallel scaling properties. We arrived at
		using Gauss-Seidel with Red and Black ordering. We ended with \(1\)st order
		discretization of the Laplacian operator, as a compromise between simplicity
		in the program and the computational efficiency of hardcoded parts of the algorithms
		dealing with the Halo, i.e. the ghost layers. It may be that the higher order discretizations
		will yield better convergence and the project has some plans to expand to incorporate it.

		Relaxation methods, such as Gauss-Seidel, work by looking for the setting up
		the equation as a diffusion equation, and then solving for the equilibrium solution.

		So suppose we want to solve the elliptic equation
		\begin{align}
			\mathcal{L}u &= \rho
			\intertext{Then we set it up as a diffusion equation}
			\pdv{u}{t} &= \mathcal{L}u - \rho
			\intertext{By starting with an initial guess for what \(u\) could be the
			equation will relax into the equilibrium solution \(\mathcal{L}u = \rho\).
			By using a Forward-Time-Centered-Space scheme to discretize, along with
			the largest stable timestep \(\Delta t = \Delta^2 / (2\cdot d))\), we
			arrive at Jacobi's method, which is an averaging of the neighbors in
			addition to a contribution from the source term. By using the already
			updated values for the calculation of the \(u^{new}\) we arrive at the
			method called Gauss-Seidel which for two dimensions is the following}
			u^{n+1}_{i,j} &= \frac{1}{4}\left( u^n_{i+1,j} + u^{n +1}_{i-1,j} + u^{n}_{i, j+1} + u^{n+1}_{i,j-1}  \right) - \frac{\Delta^2 \rho_{i,j}}{4}
		\end{align}

    A slight improvement of the Gauss-Seidel algorithm is achieved by updating
    every other grid point at a time, by using Red and Black Ordering.
    This allows a vectorization of the problem and avoids any uneccessary copying.

    \subsubsection{Jacobian and Gauss-Seidel RB}
    	\label{sec:GSRB}
    	The main iterative PDE solver, in this version of the multigrid program, is a Gauss-Seidel
    	Red-Black, in addition a Jacobian solver was developed as a stepping stone and for testing purposes.
    	It is a modification of the Jacobian method, where the updated values are used where available, which lead
    	to the convergence increasing by a factor of two \citep{press_numerical_1988}.

    	Our problem is given by \(\nabla^2 \phi= -\rho\). One way to think of the Jacobian method is as
    	a diffusion problem, and with the equilibrium solution as our wanted solution. If we then discretize the
    	diffusion problem by a Forward-Time-Centralized-Space scheme, we arrive at the Jacobian method, which is shown explicitly below
    	for 1 dimension.

     	\begin{align}
    		\pdv{\phi}{t} &= \nabla^2 \phi + \rho
			\intertext{Discretizing this we obtain:}
    		\frac{\phi^{n+1}_{j} - \phi^{n+1}_{j}}{\Delta t} &= \frac{\phi^n_{j+1} - 2 \phi^n_{j} + \phi^n_{j-1}}{\Delta x^2} + \rho_j
    		\intertext{The subscript \(j\) indicates the spatial coordinate, and the superscript \(n\) is the 'temporal' component.}
    		\intertext{This is numerically stable if \( \Delta t/\Delta x^2 \le 1/2 \), so using the timestep \( \Delta t = \Delta x^2/2 \) we get}
    		\phi^{n+1}_j &= \phi^{n}_j + \frac{1}{2}\left( \phi^n_{j+1} - 2 \phi^n_{j} + \phi^n_{j-1} \right) + \frac{\Delta x^2}{2} \rho_j
    		\intertext{Then we arrive at the Jacobian method}
    		\phi^{n+1}_j &= \frac{1}{2}\left(  \phi^n_{j+1} +\phi^n_{j-1} + \Delta x^2 \rho_j \right)
    		\intertext{The Gauss-Seidel method uses updated values of \(\phi\) where they are available.}
    		\phi^{n+1}_j &= \frac{1}{2}\left(  \phi^n_{j+1} +\phi^{n+1}_{j-1} + \Delta x^2 \rho_j \right)
    	\end{align}

    	Following the same procedure, we get the Gauss-Seidel method for for \(2\)

    	\begin{equation}
    		\phi^{n+1}_{j,k} = \frac{1}{4} \left( \phi^n_{j+1,k} +\phi^{n+1}_{j-1,k} + \phi^n_{j,k+1} + \phi^{n+1}_{j,k-1} + \Delta x^2 \rho_{j,k} \right)
    	\end{equation}
		and  \(3\) dimensions.
    	\begin{equation}
    		\phi^{n+1}_{j,k,l} = \frac{1}{8} \left( \phi^n_{j+1,k,l} +\phi^{n+1}_{j-1,k,l} + \phi^n_{j,k+1,l} + \phi^{n+1}_{j,k-1,l} +
     							\phi^n_{j,k,l+1} + \phi^{n+1}_{j,k,l-1} + \Delta x^2 \rho_{j,k,l} \right)
    	\end{equation}

    	Here we have implemented a different version of the Gauss-Seidel algorithm called Red and Black ordering, which has conceptual similarities
    	to the leapfrog algorithm, where usually position and velocity is computed at \(t\) and \( t+(\delta t)/2 \). Every other grid point is labeled a
    	red point, and the remaining is black. When updating a red node only black nodes are used, and when updating black nodes only
    	red nodes are used. Then a whole cycle consists of two halfsteps which calculates the red and black nodes seperately.

    	\begin{itemize}
    		\item For all red points:
    			\[\phi^{n+1}_{j,k,l} = \frac{1}{8} \left( \phi^{n}_{j+1,k,l} +\phi^{n}_{j-1,k,l} + \phi^n_{j,k+1,l} + \phi^{n}_{j,k-1,l} +
    	 							\phi^n_{j,k,l+1} + \phi^{n}_{j,k,l-1} + \Delta x^2 \rho_{j,k,l} \right)
    			\]
    		\item For all black points:
    		\[\phi^{n+2}_{j,k,l} = \frac{1}{8} \left( \phi^{n+1}_{j+1,k,l} +\phi^{n+1}_{j-1,k,l} + \phi^{n+1}_{j,k+1,l} + \phi^{n+1}_{j,k-1,l} +
    							\phi^{n+1}_{j,k,l+1} + \phi^{n+1}_{j,k,l-1} + \Delta x^2 \rho_{j,k,l} \right)
    		\]
    	\end{itemize}

        \subsection{Restriction}
        	\label{sec:restr_simple}
        	The multigrid method (MG) has several grids of different resolution, and we need to
         	convert the problem between the diffrent grids during the overarching the MG-algorithm.
         	The restriction algorithm has the task of translating from a fine grid to a coarser grid.
			Direct insertion is the simplest way to do this, where coarse grid points
			corresponds directly to its representation on the fine grid.
        	In this implementation we chose to use a half weight stencil, which works well together
			with the \(1\) layer Halo, to restrict a quantity from a fine
        	grid to a coarse grid. A higher order restriction algorithm could later
			be implemented if thought useful. The coarse grid values is obtained by giving half weighting to
        	the fine grid point corresponding directly to the coarse grid point, and gives the remaining
        	half to the adjacent fine grid values, see \eqref{eq:restriction_stencils}, for 1D,
        	2D and 3D examples.

        	\begin{equation}
        		\begin{aligned}
        			\mathcal{R}_{1\text{D}} &= \frac{1}{4}
        			\begin{bmatrix}
        				1 & 2 & 1
        			\end{bmatrix}
        			\\
        			\mathcal{R}_{2\text{D}} &= \frac{1}{8}
        			\begin{bmatrix}
        				0 & 1 & 0
        				\\
        				1 & 4 & 1
        				\\
        				0 & 1 & 0
        			\end{bmatrix}
        			\\
        			\mathcal{R}_{3\text{D}} &= \frac{1}{12} \left(
        			\begin{bmatrix}
        				0 & 0 & 0
        				\\
        				0 & 1 & 0
        				\\
        				0 & 0 & 0
        			\end{bmatrix}
        			,
        			\begin{bmatrix}
        				0 & 1 & 0
        				\\
        				1 & 6 & 1
        				\\
        				0 & 1 & 0
        			\end{bmatrix}
        			,
        			\begin{bmatrix}
        				0 & 0 & 0
        				\\
        				0 & 1 & 0
        				\\
        				0 & 0 & 0
        			\end{bmatrix}
        			\right)
        			\label{eq:restriction_stencils}
        		\end{aligned}
        	\end{equation}

        \subsection{Prolongation}
            \label{sec:prol_simple}

            Along with the restriction operator described in the previous section, we also need prolongation
            operator to go from a coarse grid to a finer grid. As in the restriction
			operator direct insertion is the simplest algorithm. Here we will use bilinear interpolation,
			as advised in \cite{trottenberg_multigrid_2000}, for
            two dimensions and trilinear interpolation for 3 dimensions. In bilinear interpolation seperate
            linear interpolation is done in the x- y- and z-direction, then those are combined to give a result
            on the wanted spot. The same concept is expanded to give trilinear
            interpolation. The two and three dimensional stencils is given in \eqref{eq:prolongation_stencils}

            \begin{equation}
                \centering
                \begin{aligned}
                    \mathcal{P}_{2\text{D}} &= \frac{1}{4}
                    \begin{bmatrix}
                        1 & 2 & 1
                        \\
                        2 & 4 & 2
                        \\
                        1 & 2 & 1
                    \end{bmatrix}
                    \\
                    \mathcal{P}_{3\text{D}} &= \frac{1}{8} \left(
                    \begin{bmatrix}
                        1 & 2 & 1
                        \\
                        2 & 4 & 2
                        \\
                        1 & 2 & 1
                    \end{bmatrix}
                    ,
                    \begin{bmatrix}
                        2 & 4 & 2
                        \\
                        4 & 8 & 4
                        \\
                        2 & 4 & 2
                    \end{bmatrix}
                    ,
                    \begin{bmatrix}
                        2 & 2 & 1
                        \\
                        2 & 4 & 2
                        \\
                        1 & 2 & 1
                    \end{bmatrix}
                    \right)
                    \label{eq:prolongation_stencils}
                \end{aligned}
            \end{equation}
