	\subsection{Grid Structs and Partitioning}
		In this section our overall parallelization strategy is discussed as well
		as the storage needs for the grids.

	\subsubsection{Data structures}
	The fields and quantities in PINC are discretized on a \(3\)-dimensional
	grid. In the multigrid calculation
	several grids of varying spatial coarseness are used. So it will
	be useful for us to organize the data so that we have the grid stored as an
	independent structure available for the program, while the multigrid part uses
	an extended version where it also has access to different subgrids of different
	coarseness. Each multigrid struct will have an array of different subgrids,
	where the first is a pointer to the fine grid used in the rest of the calculations,
	this makes it easy to select a grid level to perform an algorithm on.

	\subsubsection{Domain partitioning}
		We have chosen to divide the physical domain onto different processors so that each takes
		care of a physical subdomain. This is known as Domain Partitioning, and suits
		our distribution algorithm as well as the multigrid method, see \cref{sec:grid_partitioning}
		for the consequences of Domain Partitioning for multigrids. Since each
		subdomain only needs to store the particles, and grids, on its physical subdomain
		the model can be upscaled in principle without any additional need for memory storage,
		by adding more processors.
		The subdomains are dependent on each other and we need
		some communication between them, which we solve by letting each subdomain
		also store the edge of the neighboring subdomain. Depending on the boundary
		conditions it could also be useful to store an extra set of values on the
		outer domain boundary as well, which will be called ghost points, \(N_G\).
		The extra grid points due the overlap between the subdomains we will call
		overlap points, \(N_O\). Let us for simplicity consider a regular domain,
		with equal size in all dimensions, with \(N\) grid points per dimension,
		\(d\) and consider how many grid values we need to store as a singular domain
		and the grid values needed when it is divided amongst several processors. Such
		a 2 dimensional case is depicted in \cref{fig:domain_part}.

		\subsection{Singular domain}
		In the case where the whole domain is worked on by one process we need \(N^d\)
		to store the values on the grid representing the physical problem, in addition
		we see that we also need to store values for the ghost points along the domain
		boundary. Given that we have one layer of ghost points on all the boundaries,
		and there is 2 boundaries per dimension, the total number of ghost points is
		given by \(N_G = 2dN\). Since there is only 1 domain we don't need to account
		for any overlap between subdomains and the total number of grid points we need to store is:

		\begin{align}
			N_{Tot} = N^d + N_G + N_O = N^d + 2dN^{d-1}
		\end{align}

		For the 2 dimensional case, shown in \cref{fig:domain_part}, that adds up to
		\(N_{Tot} = 8^2 + 2\times4\times 8 = 128\).

		\subsection{Several subdomains}
		In the case where we introduce several subdomains, in addition to storing the
		grid values and the ghost points we also need to store an overlap between the
		subdomains. If we take our whole domain \(\Omega\) and divide in up into several
		small domains \(\Omega_S\), the smaller domains only takes a subset of the
		grid points. For simplicity, and for equal load on processors, we let the
		subdomains as well be regular, with the whole domain being a multiple of the
		subdomains. Our whole domain has \(N\) grid points in each direction, if we
		then divide that domain into \(\#\Omega\) domains, then each of those subdomains
		will have \(N_S = N^d/\#\Omega\) grid points. Each of those subdomains will also
		need values representing the ghost points and overlap from the neighboring nodes.
		A boundary of a subdomain will either have overlap points, or ghost points,
		not both at the same time so for each boundary we need to 1 layer, \( N_S^{d-1}  \).
		Each subdomain will have \(2\) boundaries per dimension since we have regular subdomains.
		The total number of grid points needed per subdomain is then

		\begin{align}
			N_{Tot, S} &= N_S^d + (N_G + N_O) =  N_S^d + 2d N_S^{d-1}
			\intertext{while the total number of grid points is}
			N_{Tot} = \#\Omega N_{Tot,S}
		\end{align}

		For the 2 dimensional case discussed earlier we need \( N_{Tot,S} = 4^2 + 2\times2\times4^1 = 32\).
		Since the effect of the subdomain boundaries increases the coarser the grid is,
		we should not let the coarsest multigrid level be too small. We also don't need
		the spatial extent of the grid to be equal on all sides, but it was done here
		to keep the computations simple.

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
	%%%		Tikz picture
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\tikzstyle{vertex}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt]
	\tikzstyle{ghost}=[circle,fill=blue!25,minimum size=20pt,inner sep=0pt]
	\tikzstyle{overlap}=[circle,fill=red!50,minimum size=20pt,inner sep=0pt]

	%Note to self: This should really have been done in a more automated/smarter way
	\begin{figure}
		\centering
			\input{tikz/gridPart}
		\caption{Each circle in the figures represents 1 grid point, and the first number is the column while the second is the row. The grey colour represents physical space the computational node works on, the blue color is the outer grid points for boundary conditions and the red colour is the overlapping grid points.}
		\label{fig:domain_part}
    \end{figure}
