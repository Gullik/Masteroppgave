\section{Parallelization}
	For the parallelization of an algorithm there is two obvious issues that need
	to be addressed to ensure that the algorithm retains a high degree of parallelization;
	communication overhead and load imbalance \citep{hackbusch_multigrid_1982}.
  Communication overhead means the time the computational nodes spend communicating
  with each other, if that is longer than the actual time spent computing the speed
  of the algorithm will suffer, and load imbalance appears if some nodes need to
  do more work than others causing some nodes to stand idle.

	Here we will focus on multigrid of a 3D cubic grid, where each grid level has
  half the number of grid points. We will use grid
	partitioning to divide the domain, GS-RB (Gauss-Seidel Red-Black) as both a
	smoother and a coarse grid solver.

	We need to investigate how the different steps: interpolation, restriction,
	smoothing and the coarse grid solver, in a MG algorithm will handle parallelization.

	\subsection{Grid Partition}
		\label{sec:grid_partitioning}
		There are several well explored options for how a multigrid method can be
		parallized, for example Domain Decomposition \citep{arraras_domain_2015} (fix accent),
		Algebraic Multigrid \citep{stuben_review_2001}, see \citet{chow_survey_2006}
		for a survey of different techniques. Here we will focus on Geometric
		Multigrid (GMG) with domain decomposition used for the parallelization, as
		described in the books \cite{trottenberg_multigrid_2000, hackbusch_multigrid_1982}.

		With domain partitioning we divide the grid \(\mathcal{T}\) into geometric
		subgrids, then we can let each processes handle one subgrid each. As we will
		see it can be useful when using the GS-RB smoothing, as well as other parts
    of a PiC program, to extend the subgrids with layers of ghost cells. The GS-RB
    algorithm will directly need the adjacent nodes, in its neighbour subdomain.

	\subsection{Distributed and accumulated data}
		(Remove? Not actually relevant for our implementation)
		During the parallel execution of the code there can be useful to keep track
		of the different data structures and what needs to be accumulated over all
		the computational nodes and what only needs to be distributed on the individual computational nodes.

		\begin{itemize}
			\item \(u\) solution (\(\Phi\))
			\item \(w\) temporary correction
			\item \(d\) defect
			\item \(f\) source term (\(\rho\))
			\item \(\mathcal{L}\) differential operator
			\item \(\mathcal{P}\) interpolation operator
			\item \(\mathcal{R}\) restriction operator
			\item \( \va{u}\) Bold means accumulated vector
			\item \( \tilde{\va{ u }} \) is the temporary smoothed solution
		\end{itemize}

		\begin{itemize}
			\item Accumulated vectors:	\(\va{u}_q\), \( \hat{\va{u}}_q \), \(\tilde{\va{u}}_q\), \(\hat{\va{w}}_q\) \(\va{w}_{q-1}\), \(\va{I}\),\(\va{R}\)
			\item Distributed vectors:  \( f_q \), \(d_q\), \(d_{q-1}\)
		\end{itemize}
		% Algorithm: P is the number of processes
		% \begin{itemize}
		% 	\item If (q == 1): Solve: \(\sum_{s=1}^P\mathcal{L}_{s,1} \va{u}_1 = \sum_{s=1}^Pf_{s,1}\)
		% 	\item else:
		% 		\begin{itemize}
		% 			\item Presmooth: \( \hat{\va{u}}_q = \mathcal{S}_{pre} \va{u_q}\)
		% 			\item Compute defect: \( d_q = f_q - \mathcal{L}_q \hat{\va{u}}_q \)
		% 			\item Restrict defect: \( d_{q -1} = \mathcal{R} d_q \)
		% 			\item Initial guess:   \( \va{w}_{q-1} = 0 \)
		% 			\item Solve defect system: \( \va{w}_{q-1} = PMG(\va{w}_{q-1}, d_{q -1} ) \)
		% 			\item Interpolate correction: \( \va{w}_q = \mathcal{R} \va{w}_{q-1} \)
		% 			\item Add correction:		\( \tilde{\va{u}}_q = \hat{\va{u}}_q + \va{w}_q \)
		% 			\item Post-smooth:			\( \va{u}_q = \mathcal{S}_{post}\)
		% 		\end{itemize}
		% \end{itemize}


	\subsection{Smoothing}
		% Will use G-S with R-B ordering, supposedly good parallel properties \citep{chow_survey_2006}. Follow algorithm I in \cite{adams_distributed_2001} (alternatively try to implement the more complicated version)?

		We have earlier divided the grid into subgrids, with overlap, as described
		in subsection \ref{sec:grid_partitioning} and given each processor
		responsibility for a subgrid. Then do a a GS-RB method we start with an
		approximation of \(u^{n}_{i,j}\). Then we will obtain the next iteration by
		the following formula

		\begin{align}
			u^{n+1}_{i,j} &= \frac{1}{4}\left( u^n_{i+1,j} + u^{n +1}_{i-1,j} + u^{n}_{i, j+1} + u^{n+1}_{i,j-1}  \right) - \frac{\Delta^2 \rho_{i,j}}{4}
		\end{align}

		We can see that the for the inner subgrid we will have no problems since we
		have all the surrounding grid points. On the edges we will need the adjacent
		grid points that are kept in the other processors. To avoid the algorithm
		from asking neighboring subgrids for adjacent grid points each time it
		reaches a edge we instead update the entire neighboring column at the start.
		So we will have a 1-row overlap between the subgrids, that need to be updated
		for each iteration.


	\subsection{Restriction}
		For the transfer down a grid level, to the coarser grid we will use a half
		weighting stencil. In two dimensions it will be the following

		\begin{align}
			\mathcal{R} &= \frac{1}{8}
			\begin{bmatrix}
				0 & 1 & 0
				\\
				1 & 4 & 1
				\\
				0 & 1 & 0
			\end{bmatrix}
		\end{align}

		With the overlap of the subgrids we will have the necessary information to
		perform the restriction without needing communication between the processors
		\citep{hackbusch_multigrid_1982}.

	\subsection{Interpolation}
		For the interpolation we will use bilinear interpolation:

		\begin{align}
			\mathcal{P} &= \frac{1}{4}
			\begin{bmatrix}
				1 & 2 & 1
				\\
				2 & 4 & 2
				\\
				1 & 2 & 1
			\end{bmatrix}
		\end{align}

		Since the interpolation is always done after GS-RB iterations the the outer
		part overlapped part of the grid updated, and we can have all the necessary
		information.


	\subsection{Smoothing}
		% Will use G-S with R-B ordering, supposedly good parallel properties \citep{chow_survey_2006}. Follow algorithm I in \cite{adams_distributed_2001} (alternatively try to implement the more complicated version)?

		We have earlier divided the grid into subgrids, with overlap, as described
		in subsection \ref{sec:grid_partitioning} and given each processor
		responsibility for a subgrid. Then do a a GS-RB method we start with an
		approximation of \(u^{n}_{i,j}\). Then we will obtain the next iteration by
		the following formula

		\begin{align}
			u^{n+1}_{i,j} &= \frac{1}{4}\left( u^n_{i+1,j} + u^{n +1}_{i-1,j} + u^{n}_{i, j+1} + u^{n+1}_{i,j-1}  \right) - \frac{\Delta^2 \rho_{i,j}}{4}
		\end{align}

		We can see that the for the inner subgrid we will have no problems since we
		have all the surrounding grid points. On the edges we will need the adjacent
		grid points that are kept in the other processors. To avoid the algorithm
		from asking neighboring subgrids for adjacent grid points each time it
		reaches a edge we instead update the entire neighboring column at the start.
		So we will have a 1-row overlap between the subgrids, that need to be updated
		for each iteration.


	\subsection{Restriction}
		For the transfer down a grid level, to the coarser grid we will use a half
		weighting stencil. In two dimensions it will be the following

		\begin{align}
			\mathcal{R} &= \frac{1}{8}
			\begin{bmatrix}
				0 & 1 & 0
				\\
				1 & 4 & 1
				\\
				0 & 1 & 0
			\end{bmatrix}
		\end{align}

		With the overlap of the subgrids we will have the necessary information to
		perform the restriction without needing communication between the processors
		\citep{hackbusch_multigrid_1982}.

	\subsection{Interpolation}
		For the interpolation we will use bilinear interpolation:

		\begin{align}
			\mathcal{P} &= \frac{1}{4}
			\begin{bmatrix}
				1 & 2 & 1
				\\
				2 & 4 & 2
				\\
				1 & 2 & 1
			\end{bmatrix}
		\end{align}

		Since the interpolation is always done after GS-RB iterations the the outer
		part overlapped part of the grid updated, and we can have all the necessary
		information.


	\subsection{Scaling}
		\subsubsection{Volume-Boundary effect}
		While a sequential MG algorithm has a theoretical scaling of \(\order{N}\)
		\citep{press_numerical_1988}, where \(N\) is the number of grid points, an
		implementation will have a lower scaling efficiency due to interprocessor
		communication. We want a parallel algorithm that attains a high speedup with
		more added processors \(P\),  compared to sequential \(1\) processor algorithm.
		Let \(T(P)\) be the computational time needed for solving the problem on \(P\)
		processors. Then we define the speedup \(S(P)\) and the parallel efficiency \(E(P)\) as

		\begin{align}
			S(P) = \frac{T(1)}{T(P)} \qquad E(P) = \frac{S(P)}{P}
		\end{align}

		A perfect parallel algorithm would the computational time would scale inversely
		with the number of processors, \(T(P) \propto 1/P\) leading to \( E(P) =1 \).
		Due to the necessary interprocessor communication that is generally not
		achievable. The computational time of the algorithm is also important, if
		the algorithm is very slow but has good parallel efficiency it is often
		worse than a fast algorithm with a worse parallel efficiency.

		The parallel efficiency of an algorithm is governed by the ratio between the
		time of communication and computation, \(T_{comm}/T_{comp}\). If there is no
		need for communication, like on \(1\) processor, the algorithm is perfectly
		parallel efficient. In our case the whole grid is diveded into several subgrids,
		which is assigned to different processors. In many cases the time used for
		computation is roughly scaling with the interior grid points, while the
		communication time is scaling with the boundaries of the subgrids. If a
		local solution method is used on a local problem it is only the grid points
		at the boundary that needs the information from grid points on the other
		processors. Since the edges has lower dimensionality than the inner grid
		points. So when the problem is is increasing the time for computation grows
		faster than the time for communication, and a parallel algorithm often has a
		higher parallel efficiency on a larger problem. This is called the Boundary-Volume effect.

		\subsubsection{Parallel complexity}
			The complexities, as in needed computational work, of sequential and
			parallel MG cycles is calculated in \cite{hackbusch_multigrid_1982} and is
			shown in \cref{tab:parallel_complexity}. In the table we can see that in
			the parallel case there is a substantial increase in the complexity in the
			case of \(W\) cycles compared to \(V\) cycles. In the sequential case the
			change in complexity when going to a \(W\) cycle is not dependent on the
			problem size, but it is in the parallel case.


		\begin{table}
			\centering
			\begin{tabular}{ c  c c c}
				& Cycle & Sequential & Parallel
			  	\\  \hline
			  	MG & V & \(\order{N}\) & \(\order{\log N}\)
			  	\\
			  	& W & $\order{N}$ & $\order{\sqrt{N}}$
			  	\\ \hline
				FMG & V & $\order{N}$ & $\order{\log^2 N}$
				\\
				& W & $\order{N}$ & \( \order{\sqrt{N} \log{N}} \)
			\end{tabular}
			\caption{The parallel complexities of sequential and parallel multigrid cycles}
			\label{tab:parallel_complexity}
		\end{table}
